a
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
a[2] = 4
a
a[3] = 4
a
a[4] = 4
a
1:4
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
?svm
svmr.pred = predict(svc.fit, test_set)
err[i] = sum(!(svmr.pred == test_set$mpg01))/nrow(test_set)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
?tune
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(tune.out)
?svm
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(svc.tune)
summary(svc.tune.best)
summary(svc.tune$best.model)
svc.tune$best.model
svc.tune$best.model
summary(svc.tune$best.model)
svc.tune$best.model
summary(svc.tune)
svc.tune$best.parameters
svc.tune$best.performance
svc.tune$best.model
# SVM (polynomial)
cost_vec = c(0.01, 0.1, 1, 5, 10, 100)
gamma_vec = c(0.5, 1, 2, 3, 4)
degree_vec = c(1, 2, 3, 4, 5)
svmp.tune = tune(svm,
mpg01 ~ .,
data = Auto,
kernel = "polynomial",
ranges = list(cost = cost_vec,
gamma = gamma_vec,
degree = degree_vec))
k_vec = c(1,2,3,4,5,6,7,8,9,10)
knn.tune = tune(knn,
mpg01 ~ .,
Auto,
ranges = list(k = k_vec))
knn.tune = tune(knn,
mpg01 ~ .,
data = Auto,
ranges = list(k = k_vec))
k_vec = c(1,2,3,4,5,6,7,8,9,10)
knn.tune = tune(knn,
mpg01 ~ .,
data = Auto,
ranges = list(k = k_vec))
k_vec = c(1,2,3,4,5,6,7,8,9,10)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
k_vec = c(1,2,3,4,5,6,7,8,9,10, 11, 12, 13, 14, 15, 16, 17)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
k_vec = c(1,5,10, 15, 20, 25, 30)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
k_vec = c(1,5,10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
k_vec = c(1,5,10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
summary(knn.tune$best.parameters)
summary(knn.tune$best.model)
summary(knn.tune$best.performance)
K = 10 # Number of nearest neighbours
err = numeric(k)
for (i in 1:k) {
test_indexes = which(folds == i, arr.ind = TRUE)
test.X = data.matrix(Auto[test_indexes, -match("mpg01", names(Auto))])
train.X = data.matrix(Auto[-test_indexes, -match("mpg01", names(Auto))])
train.mpg01 = data.matrix(mpg01[-test_indexes,])
test.mpg01 = as.factor(data.matrix(mpg01[test_indexes,]))
knn.pred = knn(train.X, test.X, train.mpg01, k = K)
err[i] = sum(!(knn.pred == test.mpg01))/nrow(test_set)
}
knn.cv_error = mean(err)
knn.cv_error
# # SVM (polynomial)
# cost_vec = c(0.01, 0.1, 1, 5, 10, 100)
# gamma_vec = c(0.5, 1, 2, 3, 4)
# degree_vec = c(1, 2, 3, 4, 5)
# svmp.tune = tune(svm,
#                  mpg01 ~ .,
#                  data = Auto,
#                  kernel = "polynomial",
#                  ranges = list(cost = cost_vec,
#                                gamma = gamma_vec,
#                                degree = degree_vec))
# Features to include in the analysis
features = names(Auto) %in% c("acceleration", "mpg01")
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
View(tune.out)
# K-nearest neighbours
K = 10 # Number of nearest neighbours
err = numeric(k)
for (i in 1:k) {
test_indexes = which(folds == i, arr.ind = TRUE)
test.X = data.matrix(Auto[test_indexes, -match("mpg01", names(Auto))])
train.X = data.matrix(Auto[-test_indexes, -match("mpg01", names(Auto))])
train.mpg01 = data.matrix(mpg01[-test_indexes,])
test.mpg01 = as.factor(data.matrix(mpg01[test_indexes,]))
knn.pred = knn(train.X, test.X, train.mpg01, k = K)
err[i] = sum(!(knn.pred == test.mpg01))/nrow(test_set)
}
knn.cv_error = mean(err)
summary(knn.tune)
k_vec = c(1, 2, 3, 5,10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
summary(knn.tune$best.parameters)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
library(ISLR)
library(MASS)
library(class)
library(e1071)
Auto = na.omit(Auto)
cor(Auto[!names(Auto) %in% "name"])
mpg01 = data.frame(Auto$mpg > median(Auto$mpg))
Auto$mpg = NULL
Auto["mpg01"] = mpg01
Auto$mpg01 = as.factor(Auto$mpg01)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
library(ISLR)
library(MASS)
library(class)
library(e1071)
Auto = na.omit(Auto)
cor(Auto[!names(Auto) %in% "name"])
# Indicates that the weight, displacement and horsepower have the strongest correlations with mpg
mpg01 = data.frame(Auto$mpg > median(Auto$mpg))
Auto$mpg = NULL
Auto["mpg01"] = mpg01
Auto$mpg01 = as.factor(Auto$mpg01)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(knn.tune)
# LDA
err = numeric(k)
for (i in 1:k) {
test_indexes = which(folds == i, arr.ind = TRUE)
test_set = Auto[test_indexes,]
train_set = Auto[-test_indexes,]
lda.fit = lda(mpg01 ~ ., data = Auto, subset = (folds != i))
ida.pred = predict(lda.fit, test_set)
err[i] = sum(!(ida.pred$class == test_set$mpg01))/nrow(test_set)
}
lda.cv_error = mean(err)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 30, 50, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
# K-nearest neighbours
k_vec = c(1, 2, 3, 4, 5, 10, 15, 20, 30, 50, 100)
knn.tune = tune.knn(Auto[,-match("mpg01", names(Auto))],
Auto[,match("mpg01", names(Auto))],
k = k_vec)
summary(knn.tune)
View(qda.pred)
?seed
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
lda.cv''
lda.cv_error
qdaa.cv_error
qda.cv_error
summary(knn.tune)
summary(svc.tune)
summary(rsvm.tune)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(svmr.tune)
summary(svmr.tune$best.parameters)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(svmr.tune)
lda.fit = lda(mpg01 ~ ., data = Auto, subset = (folds != i))
# LDA
err = numeric(k)
for (i in 1:k) {
test_indexes = which(folds == i, arr.ind = TRUE)
test_set = Auto[test_indexes,]
train_set = Auto[-test_indexes,]
lda.fit = lda(mpg01 ~ ., data = Auto, subset = (folds != i))
ida.pred = predict(lda.fit, test_set)
err[i] = sum(!(ida.pred$class == test_set$mpg01))/nrow(test_set)
}
integer(4)
# SVM (radial)
set.seed(1)
cost_vec = c(0.01, 0.1, 1, 5, 10)
gamma_vec = c(0.5, 1, 2, 3, 4)
svmr.tune = tune(svm,
mpg01 ~ .,
data = Auto,
kernel = "radial",
ranges = list(cost = cost_vec, gamma = gamma_vec))
# summary(svmr.tune)
summary(svmr.tune)
# # SVM (polynomial)
set.seed(1)
cost_vec = c(0.01, 0.1, 1, 5, 10)
degree_vec = c(1, 2, 3, 4, 5)
svmp.tune = tune(svm,
mpg01 ~ .,
data = Auto,
kernel = "polynomial",
ranges = list(cost = cost_vec,
degree = degree_vec))
summary(svmp.tune)
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
summary(knn.tune)
lda.fit
qda.fit
lda.fit
# Only keep weight, displacement and mpg01 in Auto
features = names(Auto) %in% c("weight", "displacement", "mpg01")
Auto = Auto[features]
source('C:/Users/Oscar Bergqvist/Desktop/SF2935-ISL/project1.R')
lda.fit
men <- read.csv("train.csv")
library("car")
library("dplyr")
library("tidyverse")
library("corrplot")
library("leaps")
library("MASS")
library("glmnet")
setwd("C:/Users/NextLevel/Desktop/SF2930-projects/project-1")
# Read data and fit linear model to data
men <- read.csv("train.csv")
# Read data and fit linear model to data
men <- read.csv("train.csv")
install.packages("ggplot2")
install.packages("foreach")
install.packages("xlsx")
library(ggplot2)
library(foreach)
library(xlsx)
# load packages
library("car")
library("dplyr")
library("tidyverse")
library("corrplot")
# load packages
library("car")
library("dplyr")
# load packages
library("car")
library("tidyverse")
library("corrplot")
library("leaps")
library("MASS")
rnorm(100*20)
rnorm(10)
setwd("C:/Users/NextLevel/Desktop/SF2930-projects/project-1")
library("car")
install.packages("car")
library("dplyr")
library("tidyverse")
install.packages("tidyverse")
library("corrplot")
library("tidyverse")
R-version
R.version
library("car")
library("dplyr")
library("tidyverse")
library("corrplot")
library("leaps")
library("MASS")
library("glmnet")
setwd("C:/Users/Oscar Bergqvist/Desktop/SF2930-projects/project-1")
SI <- function(men){
men$weight <- c(0.001*453.6*men$weight)
men$height <- c(2.54*0.01*men$height)
men$neck <- c(0.01*men$neck)
men$chest <- c(0.01*men$chest)
men$abdomen <- c(0.01*men$abdomen)
men$hip <- c(0.01*men$hip)
men$thigh <- c(0.01*men$thigh)
men$knee <- c(0.01*men$knee)
men$ankle <- c(0.01*men$ankle)
men$biceps <- c(0.01*men$biceps)
men$forearm <- c(0.01*men$forearm)
men$wrist <- c(0.01*men$wrist)
return(men)
}
res <- function(model_men) {
n = nrow(model_men$model)
p = ncol(model_men$model) - 1
h_ii = lm.influence(model_men)$hat
hat_mean <- 2*p/n
res = residuals(model_men)
MS_res = sum(res^2)/(n-p)
res_std   = res / sqrt(MS_res)
res_stud  = res / sqrt(MS_res*(1-h_ii))
res_press = res / (1 - h_ii)
S_sq = ((n-p)*MS_res - res^2/(1-h_ii)) / (n-p-1)
res_rstud = res / sqrt(S_sq*(1-h_ii))
return(list("res"=res,
"res_std"=res_std,
"res_stud"=res_stud,
"res_rstud"=res_rstud,
"res_press"=res_press))
}
# Read data
men <- read.csv("train.csv")
men_test <- read.csv("test.csv")
men$X <- NULL
men_test$X <- NULL
# Initial model
model_men <- lm(density ~ ., data = men)
anova(model_men)
summary(model_men)
plot(model_men)
### Residual analysis on the full model ###
res <- res(model_men)
plot(res$res)
plot(res$res_stud)
plot(res$res_rstud)
plot(res$res_press)
jmf <- res$res - res$res_press
plot(jmf)
# Leverage points
n = nrow(model_men$model)
p = ncol(model_men$model) - 1
h_ii = lm.influence(model_men)$hat
hat_mean <- 2*p/n
leverage <- which(h_ii > hat_mean)
# Influential points using COVRATIO
influential <- which(covratio(model_men) > 1+3*p/n | covratio(model_men) < 1-3*p/n)
# Influential + leverage points
lev_infl <- intersect(leverage, influential)
# Using this set of observation, manually look at each one to determine which are to be removed
men <- men[-c(30, 124, 178),]
# Fit linear model to reduced model
model_men <- lm(density ~ ., data = men)
anova(model_men)
summary(model_men)
plot(model_men)
cor %>%
corrplot.mixed()
plot(model_men)
# Visualize correlation between different explanatory variables
men %>%
dplyr::select(age,weight,height,neck,chest,abdomen,hip,thigh,knee,ankle,biceps,forearm,wrist) %>%
cor %>%
corrplot.mixed()
# Spectral Decmoposition
X <- data.matrix(men[c(-1)], rownames.force = NA)
X <- scale(X)
X_prim <- t(X)
X_corr <- X_prim %*% X
eigen <- eigen(X_corr, symmetric = TRUE, only.values = FALSE, EISPACK = FALSE)
lambda_max <- max(eigen$values)
lambda_min <- min(eigen$values)
k <- lambda_max %/% lambda_min
# Calculate VIF (if VIF_j > 10, we have a problem)
vif <- vif(model_men)
# MultikolinjÃ¤ritet + model selection mha LEAPS
y <- data.matrix(men[c(1)])
x <- data.matrix(men[c(-1)])
leaps <- leaps(x, y, method="adjr2", nbest = 1)
fold_width <- floor(nrow(men)/10)
adjR_sq_models <- NULL
for(model_ind in 1:13){
model <- leaps$which[model_ind,]
adjR_sq <- NULL
for(fold_ind in 0:9){
val_ind <- (fold_width*fold_ind+1):(fold_width*fold_ind + fold_width)
subset_vec <- rep(TRUE, nrow(men))
subset_vec[val_ind] <- FALSE
model_cv <- lm(formula(men[c(TRUE, model)]), data = men, subset = subset_vec)
pfit <- predict(model_cv, newdata = men[val_ind,])
y_val = men[val_ind,"density"]
SS_res <- sum((y_val-pfit)^2)
SS_R <- sum((pfit-mean(y_val))^2)
SS_T <- SS_R + SS_res
adjR_sq <- c(adjR_sq, SS_R / SS_T)
}
adjR_sq_models <- c(adjR_sq_models, mean(adjR_sq))
}
rm(SS_res, SS_R, SS_T, adjR_sq, y_val, subset_vec, model_cv,
pfit, model, fold_width, fold_ind, model_ind, val_ind)
best_model <- leaps$which[which.max(adjR_sq_models),]
best_model <- lm(formula(men[c(TRUE, best_model)]), data = men)
# CP
y <- data.matrix(men[c(1)])
x <- data.matrix(men[c(-1)])
leaps <- leaps(x, y, method="Cp")
plot(rowSums(leaps$which,2), leaps$Cp, xlim = c(0,14), ylim = c(0,14))
abline(0, 1)
text(rowSums(leaps$which,2), leaps$Cp, 1:length(rowSums(leaps$which,2))) # Kandidater: nr. 32, 48
anova(model_men_Cp)
summary(model_men_Cp)
# Vi prÃ¶var nr 32
model_men_Cp <- lm(density ~ weight + abdomen + biceps + wrist, data = men)
model_men_Cp
anova(model_men_Cp)
summary(model_men_Cp)
# Vi prÃ¶var nr 32
model_men_Cp <- lm(density ~ weight + abdomen + biceps + wrist, data = men)
anova(model_men_Cp)
summary(model_men_Cp)
plot(model_men_Cp)
studres(model_men_Cp)
# MultikolinjÃ¤ritet + model selection mha LASSO
y <- as.matrix(men[c(1)])
x <- as.matrix(men[c(-1)])
# VÃ¤lj optimala lambda genom att iterera 100 ggr Ã¶ver 10-fold modeller
lambdas = NULL
for (i in 1:50)
{
fit <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
errors = data.frame(fit$lambda, fit$cvm)
lambdas <- rbind(lambdas, errors)
print("*")
}
# take mean cvm for each lambda
lambdas <- aggregate(lambdas[, 2], list(lambdas$fit.lambda), mean)
# select the best one
bestindex = which(lambdas[2]==min(lambdas[2]))
bestlambda = lambdas[bestindex,1]
# and now run glmnet once more with it
lasso <- glmnet(x, y, alpha = 1, lambda=bestlambda)
pfit <- predict.glmnet(lasso, x[1:5,], s = 0, type="response")
plot(pfit,y)
plot(pfit,y)
pfit <- predict.glmnet(lasso, x[1:5,], s = 0, type="response")
plot(pfit,y)
# MultikolinjÃ¤ritet + model selection mha LASSO
y <- as.matrix(men[c(1)])
plot(pfit,y)
r2_lasso <- lasso$dev.ratio
model_men <- fit
y_test <- as.matrix(men_test[c(1)])
x_test <- as.matrix(men_test[c(-1)])
pfit <- predict.glmnet(lasso, x_test, s=0, type="response")
SS_res <- sum((y_test-pfit)^2)
SS_R <- sum((pfit-mean(y_test))^2)
SS_T <- SS_R + SS_res
R_sq_lasso <- SS_R / SS_T
pfit <- predict(model_men_Cp, as.data.frame(x_test))
SS_res <- sum((y_test-pfit)^2)
SS_R <- sum((pfit-mean(y_test))^2)
SS_T <- SS_R + SS_res
R_sq_bestsub <- SS_R / SS_T
#Predict:a pÃ¥ fÃ¶rsta modellen (all possible regressions)
p_apr <- predict(model_men_Cp, men_test)
x.test <- model.matrix(density ~ ., men_test)[,-1]
pred <- predict(lasso, type='response', newx=x.test)
men_test$density <- NULL
X <- data.matrix(men_test)
pfit <- predict.glmnet(lasso, X, s = 0, type="response")
